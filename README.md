ðŸ§® Lagrangian-Based Backpropagation in NumPy

This project implements a basic feedforward neural network with a custom Lagrangian-based backward propagation algorithm using pure NumPy. Itâ€™s designed for learners, researchers, and enthusiasts interested in the mathematical foundations of deep learning, particularly in exploring alternatives to standard gradient backpropagation.

â¸»

ðŸ“š Motivation

Most neural networks today use gradient descent and automatic differentiation to update weights. But what happens when you manually define a Lagrangian and derive backpropagation from physical or optimization principles?

This project answers that question with:
	â€¢	A 2-layer neural network (input â†’ hidden â†’ output)
	â€¢	Manual computation of gradients via Lagrangian-based updates
	â€¢	Minimal use of external libraries for transparency

â¸»

ðŸ§  Features
	â€¢	Custom forward pass using matrix multiplication
	â€¢	Lagrangian-based backward pass with analytically derived updates
	â€¢	Training loop with weight updates via manual gradient application
	â€¢	Configurable architecture (input size, hidden size, learning rate)
	â€¢	Written entirely in NumPy for educational clarity
